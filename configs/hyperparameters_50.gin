## Hyperparameters used in the machine learning methods, when 50 features are used

## Import the python methods files to access the classes
import methods.models
import methods.dnn
import gin.tf.external_configurables


## AdaBoost Classifier - Hyperparameters
ada_boost.learning_rate = 1                     # float, shrinks the contribution of each classifier
ada_boost.n_estimators = 70                     # int, max number of estimators (trees in the forest) at which boosting is terminated


## Decision Tree Classifier - Hyperparameters
decision_tree.criterion = 'entropy'             # {'gini', 'entropy'}, the quality of the split
decision_tree.max_depth = 10                    # int, the maximum depth of the tree, default at None
decision_tree.splitter = 'best'                 # {'best', 'random'}, strategy on how the node split is chosen
decision_tree.max_features = None               # {'None', 'auto', 'sqrt', 'log2'}, the number of features to consider when splitting
decision_tree.min_samples_split = 3             # int, minimum required samples to split a node
dnn.class_weight = {0:1, 1: 3}                  # dict, weights associated with the classes


## Random forest Classifier - Hyperparameters
random_forest.n_estimators = 60                 # int, Number of trees in the forest
random_forest.criterion = 'entropy'             # {“gini”, “entropy”}, function to measure the quality of a split
random_forest.max_depth = 12                    # int, maximum depth of the tree
random_forest.max_features = "auto"             # {“auto”, “sqrt”, “log2”}, max number of features considered for splitting a node
random_forest.min_samples_split = 2             # int, minimum required samples to split a node
random_forest.class_weight = {0:1, 1: 3}        # dict, weights associated with the classes


## Deep Neural Network - Hyperparameters
# Parameters to the model creation
dnn.dropout = 0.2                               # float, rate of how many input units which is set to 0

# Parameters to the fitting
dnn.epochs = 200                                # Number of training epochs
dnn.batch_size = 60                             # int, Number of samples per gradient update
dnn.validation_split = 0.33                     # float, the percentage of the training data which should be used to validation when training

# Parameters to the compilation
dnn.optimizer_cls = @tf.train.AdamOptimizer()   # tf.optimizer, the optimization function
dnn.loss = 'binary_crossentropy'                # string, default None, the loss function
dnn.class_weight = {0:1, 1: 3}                  # dict, weights associated with the classes
tf.train.AdamOptimizer.learning_rate = 0.001    # Learning rate to the optimizer
